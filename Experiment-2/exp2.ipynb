{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights and bias for hidden perceptrons:\n",
      "p1: [ 0.1 -0.2] 0.0\n",
      "p2: [-0.1  0.1] 0.0\n",
      "p3: [0.1 0.1] 0.0\n",
      "p4: [-0.1 -0.1] 0.1\n",
      "\n",
      "Learned weights and bias for final XOR perceptron:\n",
      "xor: [0.1 0.1 0.  0. ] 0.0\n",
      "\n",
      "XOR MLP Results:\n",
      "Input: [0 0] -> XOR Output: 0\n",
      "Input: [0 1] -> XOR Output: 1\n",
      "Input: [1 0] -> XOR Output: 1\n",
      "Input: [1 1] -> XOR Output: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Returns a binary output (0 or 1) for each row in 'inputs'\n",
    "    based on the sign of (inputs . weights + bias).\n",
    "    \"\"\"\n",
    "    net = np.dot(inputs, weights) + bias\n",
    "    return np.where(net > 0, 1, 0)\n",
    "\n",
    "def train_perceptron(X, y, epochs=50, lr=0.1):\n",
    "    \"\"\"\n",
    "    Trains a single-layer perceptron using the classic perceptron rule.\n",
    "    - X: 2D NumPy array of input samples (rows = samples, columns = features).\n",
    "    - y: 1D NumPy array of target outputs (0 or 1).\n",
    "    - epochs: Number of times to iterate over the entire training set.\n",
    "    - lr: Learning rate for weight updates.\n",
    "\n",
    "    Returns the learned weights and bias.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0.0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for i in range(n_samples):\n",
    "            # Current sample\n",
    "            x_i = X[i]\n",
    "            target = y[i]\n",
    "            \n",
    "            # Perceptron output\n",
    "            output = 1 if np.dot(x_i, w) + b > 0 else 0\n",
    "            error = target - output\n",
    "            \n",
    "            # Update rule\n",
    "            w += lr * error * x_i\n",
    "            b += lr * error\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 1) Define the 4-input XOR problem\n",
    "#    Inputs: (0,0), (0,1), (1,0), (1,1)\n",
    "#    True XOR outputs: [0, 1, 1, 0]\n",
    "# ------------------------------------------------------------------------------------\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 2) Train the 4 intermediate perceptrons (p1, p2, p3, p4)\n",
    "#    to reproduce the original \"hand-coded\" logic in your code.\n",
    "#\n",
    "#    Based on the original weights/biases, each perceptron\n",
    "#    had the following target outputs for inputs [0,0],[0,1],[1,0],[1,1]:\n",
    "#\n",
    "#    p1: w=[1,-1], b=0  => outputs: [0, 0, 1, 0]\n",
    "#    p2: w=[-1,1], b=0 => outputs: [0, 1, 0, 0]\n",
    "#    p3: w=[1,1], b=-0.5 => outputs: [0, 1, 1, 1]\n",
    "#    p4: w=[-1,-1], b=0.5 => outputs: [1, 0, 0, 0]\n",
    "# ------------------------------------------------------------------------------------\n",
    "p1_labels = np.array([0, 0, 1, 0])  # from the original p1 logic\n",
    "p2_labels = np.array([0, 1, 0, 0])  # from the original p2 logic\n",
    "p3_labels = np.array([0, 1, 1, 1])  # from the original p3 logic\n",
    "p4_labels = np.array([1, 0, 0, 0])  # from the original p4 logic\n",
    "\n",
    "# Train each hidden perceptron\n",
    "p1_w, p1_b = train_perceptron(X, p1_labels)\n",
    "p2_w, p2_b = train_perceptron(X, p2_labels)\n",
    "p3_w, p3_b = train_perceptron(X, p3_labels)\n",
    "p4_w, p4_b = train_perceptron(X, p4_labels)\n",
    "\n",
    "# Compute hidden layer outputs for each input\n",
    "h1 = perceptron(X, p1_w, p1_b)\n",
    "h2 = perceptron(X, p2_w, p2_b)\n",
    "h3 = perceptron(X, p3_w, p3_b)\n",
    "h4 = perceptron(X, p4_w, p4_b)\n",
    "\n",
    "# Stack into a single matrix: shape (4 samples, 4 features)\n",
    "hidden_output = np.stack((h1, h2, h3, h4), axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 3) Train the final XOR perceptron\n",
    "#    Original final XOR logic used w=[1,1,0,0], b=0 => effectively h1 + h2\n",
    "#    True XOR outputs are: [0,1,1,0]\n",
    "# ------------------------------------------------------------------------------------\n",
    "xor_labels = np.array([0, 1, 1, 0])\n",
    "xor_w, xor_b = train_perceptron(hidden_output, xor_labels)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# 4) Test the full network\n",
    "# ------------------------------------------------------------------------------------\n",
    "final_output = perceptron(hidden_output, xor_w, xor_b)\n",
    "\n",
    "print(\"Learned weights and bias for hidden perceptrons:\")\n",
    "print(\"p1:\", p1_w, p1_b)\n",
    "print(\"p2:\", p2_w, p2_b)\n",
    "print(\"p3:\", p3_w, p3_b)\n",
    "print(\"p4:\", p4_w, p4_b)\n",
    "\n",
    "print(\"\\nLearned weights and bias for final XOR perceptron:\")\n",
    "print(\"xor:\", xor_w, xor_b)\n",
    "\n",
    "print(\"\\nXOR MLP Results:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} -> XOR Output: {final_output[i]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
