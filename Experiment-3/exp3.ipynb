{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vrand\\AppData\\Local\\Temp\\ipykernel_20340\\332831817.py:57: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 1 Loss: 0.14476748 Accuracy: 0.9599\n",
      "Epoch: 2 Loss: 0.08885283 Accuracy: 0.97568333\n",
      "Epoch: 3 Loss: 0.06153954 Accuracy: 0.98253334\n",
      "Epoch: 4 Loss: 0.044526562 Accuracy: 0.9881167\n",
      "Epoch: 5 Loss: 0.032819714 Accuracy: 0.99113333\n",
      "Epoch: 6 Loss: 0.023686841 Accuracy: 0.9942667\n",
      "Epoch: 7 Loss: 0.017396122 Accuracy: 0.9963667\n",
      "Epoch: 8 Loss: 0.01565872 Accuracy: 0.99645\n",
      "Epoch: 9 Loss: 0.012676493 Accuracy: 0.99701667\n",
      "Epoch: 10 Loss: 0.008269353 Accuracy: 0.9986167\n",
      "Test Accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Disable eager execution to use TF1-style code (placeholders, sessions, etc.)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Prepare MNIST Data\n",
    "# ---------------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and flatten images (28x28 => 784)\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "x_test  = x_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "\n",
    "# Convert labels to one-hot vectors (10 classes)\n",
    "num_classes = 10\n",
    "y_train = np.eye(num_classes)[y_train]\n",
    "y_test  = np.eye(num_classes)[y_test]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Create Placeholders for Inputs and Labels\n",
    "# ---------------------------\n",
    "x_ph = tf.compat.v1.placeholder(tf.float32, [None, 784], name=\"x\")\n",
    "y_ph = tf.compat.v1.placeholder(tf.float32, [None, 10],  name=\"y\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define Network Architecture\n",
    "# ---------------------------\n",
    "# Hidden layer with 256 neurons\n",
    "hidden_units = 256\n",
    "W1 = tf.Variable(tf.random.normal([784, hidden_units], stddev=0.1), name=\"W1\")\n",
    "b1 = tf.Variable(tf.zeros([hidden_units]), name=\"b1\")\n",
    "\n",
    "# Output layer with 10 neurons (one for each digit)\n",
    "W2 = tf.Variable(tf.random.normal([hidden_units, 10], stddev=0.1), name=\"W2\")\n",
    "b2 = tf.Variable(tf.zeros([10]), name=\"b2\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build Feed-Forward Computation\n",
    "# ---------------------------\n",
    "# Hidden layer: weighted sum + ReLU activation\n",
    "z1 = tf.matmul(x_ph, W1) + b1\n",
    "a1 = tf.nn.relu(z1)\n",
    "\n",
    "# Output layer: weighted sum to produce logits\n",
    "logits = tf.matmul(a1, W2) + b2\n",
    "\n",
    "# Convert logits to probabilities for evaluation\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Define Loss and Back-Propagation (Training)\n",
    "# ---------------------------\n",
    "# Use the updated loss function for TF 2.x: tf.nn.softmax_cross_entropy_with_logits\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_ph, logits=logits))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Define Accuracy Metric\n",
    "# ---------------------------\n",
    "correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y_ph, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Training Parameters\n",
    "# ---------------------------\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "num_batches = x_train.shape[0] // batch_size\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Train the Neural Network\n",
    "# ---------------------------\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # (Optional) Shuffle training data at the start of each epoch\n",
    "        indices = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            sess.run(optimizer, feed_dict={x_ph: batch_x, y_ph: batch_y})\n",
    "        \n",
    "        # Evaluate training performance after each epoch\n",
    "        train_loss, train_acc = sess.run([loss, accuracy], feed_dict={x_ph: x_train, y_ph: y_train})\n",
    "        print(\"Epoch:\", epoch+1, \"Loss:\", train_loss, \"Accuracy:\", train_acc)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_acc = sess.run(accuracy, feed_dict={x_ph: x_test, y_ph: y_test})\n",
    "    print(\"Test Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
